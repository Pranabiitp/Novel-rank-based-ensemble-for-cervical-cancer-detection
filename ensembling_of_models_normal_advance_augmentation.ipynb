{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98abc359",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,glob\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "gpu=int(input(\"Which gpu number you would like to allocate:\"))\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=str(gpu)\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import re\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as stats\n",
    "import datetime\n",
    "import keras\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "import numpy as np\n",
    "#from keras_cv.layers import RandomCutout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.applications.xception import preprocess_input\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import radon, rescale\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from classification_models.keras import Classifiers\n",
    "from skimage import feature\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as rp\n",
    "from classification_models.keras import Classifiers\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xp\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from keras.utils import get_file\n",
    "import os,glob\n",
    "from tensorflow.compat.v1 import InteractiveSession\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from classification_models.keras import Classifiers\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "#import clahe\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sympy.solvers import solve\n",
    "from sympy import Symbol\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,Layer,ReLU, MaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "import pickle\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import f1_score\n",
    "import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from skimage.transform import radon, rescale\n",
    "from skimage.filters import roberts, sobel, scharr, prewitt\n",
    "from skimage import feature\n",
    "import os,glob\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import argparse\n",
    "import re\n",
    "import datetime\n",
    "from tensorflow.keras.layers import  Input,Conv2D,BatchNormalization,Activation,Subtract,LeakyReLU,Add,Average,Lambda,MaxPool2D,Dropout,UpSampling2D,Concatenate,Multiply,GlobalAveragePooling2D,Dense,ZeroPadding2D,AveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate,Flatten,ConvLSTM2D,LayerNormalization,GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import CSVLogger, ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import Sequential\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from skimage import data, exposure\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf   \n",
    "import keras\n",
    "from classification_models.keras import Classifiers\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve,auc\n",
    "from sklearn.metrics import f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import loadtxt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skimage.feature import hog,local_binary_pattern\n",
    "from tensorflow.keras.metrics import Recall, Precision\n",
    "from skimage import data, exposure\n",
    "from tensorflow.keras.layers import Layer\n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from sklearn.utils import shuffle\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "import cv2\n",
    "IM_SIZE=224\n",
    "#import tensorflow_probability as tfp\n",
    "\n",
    "\n",
    "def no_data_augmentation(cat1_files,cat2_files,cat3_files,cat4_files,cat5_files):\n",
    "    aug_cat1=[]\n",
    "    aug_cat2=[]\n",
    "    aug_cat3=[]\n",
    "    aug_cat4=[]\n",
    "    aug_cat5=[]\n",
    "    for ele in cat1_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele)\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_cat1.append(pic)\n",
    "    for ele in cat2_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele)\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_cat2.append(pic)\n",
    "    for ele in cat3_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele)\n",
    "        x = asarray(x)\n",
    "      \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_cat3.append(pic)\n",
    "    \n",
    "    for ele in cat4_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele)\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_cat4.append(pic)    \n",
    "    for ele in cat5_files:\n",
    "        #ele=ele/255.0\n",
    "        x = Image.open(ele)\n",
    "        x = asarray(x)\n",
    "        \n",
    "        pic = cv2.resize(x,(224,224),interpolation = cv2.INTER_CUBIC)\n",
    "        pic=pic/255.0\n",
    "        aug_cat5.append(pic)   \n",
    "    for i in range(len(aug_cat1)):\n",
    "        aug_cat1[i]=aug_cat1[i].reshape((224,224,3))\n",
    "    \n",
    "    for i in range(len(aug_cat2)):\n",
    "        aug_cat2[i]=aug_cat2[i].reshape((224,224,3))\n",
    "    for i in range(len(aug_cat3)):\n",
    "        aug_cat3[i]=aug_cat3[i].reshape((224,224,3))\n",
    "    for i in range(len(aug_cat4)):\n",
    "        aug_cat4[i]=aug_cat4[i].reshape((224,224,3))    \n",
    "    for i in range(len(aug_cat5)):\n",
    "        aug_cat5[i]=aug_cat5[i].reshape((224,224,3)) \n",
    "    \n",
    "    print(\"Category 1 files without augmentation:\",len(aug_cat1))\n",
    "    print(\"Category 2 files without augmentation:\",len(aug_cat2))\n",
    "    print(\"Category 3 files without augmentation:\",len(aug_cat3))\n",
    "    print(\"Category 4 files without augmentation:\",len(aug_cat4))\n",
    "    print(\"Category 5 files without augmentation:\",len(aug_cat5))\n",
    "    return aug_cat1,aug_cat2,aug_cat3, aug_cat4, aug_cat5\n",
    "\n",
    "\n",
    "\n",
    "def box(lamda):\n",
    "    IM_SIZE=224\n",
    "    r_x=int(np.random.uniform(0, IM_SIZE))\n",
    "    \n",
    "    \n",
    "    r_y=int(np.random.uniform(0, IM_SIZE))\n",
    "    \n",
    "    r_w=IM_SIZE*np.sqrt(1 - lamda)\n",
    "    r_h=IM_SIZE*np.sqrt(1 - lamda)\n",
    "\n",
    "    r_x=np.clip(r_x - r_w // 2, 0, IM_SIZE)\n",
    "    r_y=np.clip(r_y-r_h//2, 0, IM_SIZE)\n",
    "\n",
    "    x_b_r=np.clip(r_x+r_w//2,0, IM_SIZE)\n",
    "    y_b_r=np.clip(r_y+r_h//2,0, IM_SIZE)\n",
    "\n",
    "    r_w=y_b_r-r_y\n",
    "    if(r_w==0):\n",
    "        r_w=1\n",
    "    r_h=y_b_r-r_y\n",
    "    if(r_h==0):\n",
    "        r_h=1\n",
    "     \n",
    "    return int(r_y),int(r_x),int(r_h),int(r_w)\n",
    "\n",
    "  \n",
    "def cutmix(image1,label1,images,labels):\n",
    "    np.random.seed(None)\n",
    "    index = np.random.permutation(len(images))\n",
    "    lamda=stats.beta(0.4, 0.4).rvs()\n",
    "    r_y,r_x,r_h,r_w=box(lamda)\n",
    "    image2 = images[index[0]]\n",
    "    label2 = labels[index[0]]\n",
    "    crop2=tf.image.crop_to_bounding_box(image2,r_y,r_x,r_h,r_w)\n",
    "    pad2=tf.image.pad_to_bounding_box(crop2,r_y,r_x,IM_SIZE,IM_SIZE)\n",
    "    crop1=tf.image.crop_to_bounding_box(image1,r_y,r_x,r_h,r_w)\n",
    "    pad1=tf.image.pad_to_bounding_box(crop1,r_y,r_x,IM_SIZE,IM_SIZE)\n",
    "    image=image1-pad1+pad2\n",
    "    lamda=1-(r_h*r_w)/(IM_SIZE*IM_SIZE)\n",
    "    label=lamda*label1+(1-lamda)*label2\n",
    "    return image,label\n",
    "\n",
    "def on_hot_encode_labels(lables):\n",
    "    aug_list=[]\n",
    "    for i in range(len(lables)):\n",
    "        if lables[i]==0:\n",
    "            aug_list.append([0,1,0,0,0])\n",
    "        elif lables[i]==1:\n",
    "            aug_list.append([1,0,0,0,0])\n",
    "        elif lables[i]==2:\n",
    "            aug_list.append([0,0,1,0,0])\n",
    "        elif lables[i]==3:\n",
    "            aug_list.append([0,0,0,1,0])\n",
    "        elif lables[i]==4:\n",
    "            aug_list.append([0,0,0,0,1])\n",
    "    return aug_list\n",
    "import cv2\n",
    "IM_SIZE=224\n",
    "def mixup(image1,label1,images,labels):\n",
    "    index = np.random.permutation(len(images))\n",
    "    image2 = images[index[0]]\n",
    "    \n",
    "    label2 = labels[index[0]]\n",
    "    lamda=np.random.beta(0.4, 0.4)\n",
    "    \n",
    "    label_1=label1\n",
    "    label_2=label2\n",
    "    image=lamda*image1+(1-lamda)*image2\n",
    "    label=lamda*label_1+(1-lamda)*label_2\n",
    "    \n",
    "    return image,label\n",
    "\n",
    "\n",
    "def cutout(images,labels, pad_size=16):\n",
    "    cut_image=[]\n",
    "    cut_labels=[]\n",
    "    for index in tqdm(range(len(images))):\n",
    "        img=images[index]\n",
    "        h, w, c = img.shape\n",
    "        mask = np.ones((h + pad_size*2, w + pad_size*2, c))\n",
    "        y = np.random.randint(pad_size, h + pad_size)\n",
    "        x = np.random.randint(pad_size, w + pad_size)\n",
    "        y1 = np.clip(y - pad_size, 0, h + pad_size*2)\n",
    "        y2 = np.clip(y + pad_size, 0, h + pad_size*2)\n",
    "        x1 = np.clip(x - pad_size, 0, w + pad_size*2)\n",
    "        x2 = np.clip(x + pad_size, 0, w + pad_size*2)\n",
    "        mask[y1:y2, x1:x2, :] = 0\n",
    "        img_cutout = img * mask[pad_size:pad_size+h, pad_size:pad_size+w, :]\n",
    "        cut_image.append(img_cutout)\n",
    "        cut_labels.append(labels[index])\n",
    "    return cut_image,cut_labels\n",
    "\n",
    "def random_augment(image1,label1,images,labels):\n",
    "    datagen=ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "    )\n",
    "    x=image1.reshape((1,)+image1.shape)\n",
    "    my_images=[]\n",
    "    my_labels=[]\n",
    "    counter=0\n",
    "    for i in datagen.flow(x):\n",
    "        if counter==6:\n",
    "            break\n",
    "        my_image=i\n",
    "        my_image=my_image.reshape((224,224,3))\n",
    "        my_images.append(my_image)\n",
    "        my_labels.append(label1)\n",
    "        counter+=1     \n",
    "    \n",
    "    return my_images,my_labels\n",
    "        \n",
    "    \n",
    "def advance_data_aug(images_list,images_labels,full_data,full_label,param=2):\n",
    "    images_list=np.array(images_list)\n",
    "    images_labels=np.array(images_labels)\n",
    "    \n",
    "    # create the original array\n",
    "    arr = full_label\n",
    "\n",
    "    # define the value of the element to delete\n",
    "    value_to_delete = images_labels[0]\n",
    "    index=[]\n",
    "    for i in range(len(arr)):\n",
    "        if np.array_equal(arr[i],value_to_delete):\n",
    "            index.append(i)\n",
    "    my_full_data=[]\n",
    "    my_full_label=[]\n",
    "    for i in range(len(full_label)):\n",
    "        if i in index:\n",
    "            continue\n",
    "        else:\n",
    "            my_full_data.append(full_data[i])\n",
    "            my_full_label.append(full_label[i])\n",
    "    full_data=my_full_data.copy()\n",
    "    full_label=my_full_label.copy()\n",
    "    full_data=np.array(full_data)\n",
    "    full_label=np.array(full_label)\n",
    "    aug_list=[]\n",
    "    aug_labels=[]\n",
    "    print(\"adding original images\")\n",
    "    for i in range(len(images_list)):\n",
    "        aug_labels.append(images_labels[i])\n",
    "        aug_list.append(images_list[i])\n",
    "    print(np.array(aug_list).shape,np.array(aug_labels).shape)\n",
    "    print(\"cutmix\")\n",
    "    for i in range(2):\n",
    "        for j in tqdm(range(len(images_list))):\n",
    "            new_image,new_label=cutmix(images_list[j],images_labels[j],full_data,full_label)\n",
    "            aug_labels.append(new_label)\n",
    "            aug_list.append(new_image)\n",
    "    print(np.array(aug_list).shape,np.array(aug_labels).shape)\n",
    "    print(\"mixup\")\n",
    "    for i in range(2):\n",
    "        for j in tqdm(range(len(images_list))):\n",
    "            new_image,new_label=mixup(images_list[j],images_labels[j],full_data,full_label)\n",
    "            aug_labels.append(new_label)\n",
    "            aug_list.append(new_image)\n",
    "    print(np.array(aug_list).shape,np.array(aug_labels).shape)\n",
    "    print(\"random augmentation\")\n",
    "    for j in tqdm(range(len(images_list))):\n",
    "        new_image,new_label=random_augment(images_list[j],images_labels[j],full_data,full_label)\n",
    "        for index in range(len(new_image)):\n",
    "            aug_labels.append(new_label[index])\n",
    "            aug_list.append(new_image[index])\n",
    "    print(np.array(aug_list).shape,np.array(aug_labels).shape)\n",
    "    print(\"cutout\")\n",
    "    aug_list=np.array(aug_list)\n",
    "    aug_labels=np.array(aug_labels)\n",
    "    for i in range(2):\n",
    "        im,la=cutout(images_list,images_labels) \n",
    "        aug_list = np.concatenate([aug_list, im])\n",
    "        aug_labels=np.concatenate([aug_labels,la])\n",
    "        \n",
    "    print(np.array(aug_list).shape,np.array(aug_labels).shape)\n",
    "    return aug_list,aug_labels\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def Conv_2D_Block(x, model_width, kernel, strides=(1, 1), padding=\"same\"):\n",
    "    # 2D Convolutional Block with BatchNormalization\n",
    "    x = tf.keras.layers.Conv2D(model_width, kernel, strides=strides, padding=padding, kernel_initializer=\"he_normal\")(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.Activation('relu')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def classifier(inputs, class_number):\n",
    "    # Construct the Classifier Group\n",
    "    # inputs       : input vector\n",
    "    # class_number : number of output classes\n",
    "    out = tf.keras.layers.Dense(class_number, activation='softmax')(inputs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def regressor(inputs, feature_number):\n",
    "    # Construct the Regressor Group\n",
    "    # inputs         : input vector\n",
    "    # feature_number : number of output features\n",
    "    out = tf.keras.layers.Dense(feature_number, activation='linear')(inputs)\n",
    "    return out\n",
    "\n",
    "\n",
    "def SE_Block(inputs, num_filters, ratio):\n",
    "    squeeze = tf.keras.layers.GlobalAveragePooling2D()(inputs)\n",
    "\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters/ratio)(squeeze)\n",
    "    excitation = tf.keras.layers.Activation('relu')(excitation)\n",
    "    excitation = tf.keras.layers.Dense(units=num_filters)(excitation)\n",
    "    excitation = tf.keras.layers.Activation('sigmoid')(excitation)\n",
    "    excitation = tf.keras.layers.Reshape([1, 1, num_filters])(excitation)\n",
    "\n",
    "    scale = inputs * excitation\n",
    "\n",
    "    return scale\n",
    "\n",
    "\n",
    "def Inceptionv1_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1), padding='valid')\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1), padding='valid')\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
    "\n",
    "    branch5x5 = Conv_2D_Block(inputs, filterB3_1, (1, 1), padding='valid')\n",
    "    branch5x5 = Conv_2D_Block(branch5x5, filterB3_2, (5, 5))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch5x5, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inceptionv2_Module(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 3))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_A(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch5x5 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch5x5 = Conv_2D_Block(branch5x5, filterB2_2, (5, 5))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 3))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch5x5, branch3x3dbl, branch_pool], axis=-1, name='Inception_Block_A'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_B(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch7x7 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (1, 7))\n",
    "    branch7x7 = Conv_2D_Block(branch7x7, filterB2_2, (7, 1))\n",
    "\n",
    "    branch7x7dbl = Conv_2D_Block(inputs, filterB3_1, 1)\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (1, 7))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_2, (7, 1))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (1, 7))\n",
    "    branch7x7dbl = Conv_2D_Block(branch7x7dbl, filterB3_3, (7, 1))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch7x7, branch7x7dbl, branch_pool], axis=-1, name='Inception_Block_B'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Inception_Module_C(inputs, filterB1_1, filterB2_1, filterB2_2, filterB3_1, filterB3_2, filterB3_3, filterB4_1, i):\n",
    "    # Inception Block i\n",
    "    branch1x1 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3_2 = Conv_2D_Block(branch3x3, filterB2_2, (1, 3))\n",
    "    branch3x3_3 = Conv_2D_Block(branch3x3, filterB2_2, (3, 1))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB3_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (1, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB3_2, (3, 1))\n",
    "    branch3x3dbl_2 = Conv_2D_Block(branch3x3dbl, filterB3_3, (1, 3))\n",
    "    branch3x3dbl_3 = Conv_2D_Block(branch3x3dbl, filterB3_3, (3, 1))\n",
    "\n",
    "    branch_pool = tf.keras.layers.AveragePooling2D(pool_size=(3, 3), strides=(1, 1), padding='same')(inputs)\n",
    "    branch_pool = Conv_2D_Block(branch_pool, filterB4_1, (1, 1))\n",
    "\n",
    "    out = tf.keras.layers.concatenate([branch1x1, branch3x3_2, branch3x3_3, branch3x3dbl_2, branch3x3dbl_3, branch_pool], axis=-1, name='Inception_Block_C'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Reduction_Block_A(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
    "    # Reduction Block A (i)\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (3, 3))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "def Reduction_Block_B(inputs, filterB1_1, filterB1_2, filterB2_1, filterB2_2, filterB2_3, i):\n",
    "    # Reduction Block B (i)\n",
    "    branch3x3 = Conv_2D_Block(inputs, filterB1_1, (1, 1))\n",
    "    branch3x3 = Conv_2D_Block(branch3x3, filterB1_2, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch3x3dbl = Conv_2D_Block(inputs, filterB2_1, (1, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (1, 7))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_2, (7, 1))\n",
    "    branch3x3dbl = Conv_2D_Block(branch3x3dbl, filterB2_3, (3, 3), strides=(2, 2))\n",
    "\n",
    "    branch_pool = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2), padding='same')(inputs)\n",
    "    out = tf.keras.layers.concatenate([branch3x3, branch3x3dbl, branch_pool], axis=-1, name='Reduction_Block_'+str(i))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "class SEInception:\n",
    "    def __init__(self, length, width, num_channel, num_filters, ratio=4, problem_type='Regression',\n",
    "                 output_nums=1, pooling='avg', dropout_rate=False, auxilliary_outputs=False):\n",
    "        # length: Input Signal Length\n",
    "        # model_depth: Depth of the Model\n",
    "        # model_width: Width of the Model\n",
    "        # kernel_size: Kernel or Filter Size of the Input Convolutional Layer\n",
    "        # num_channel: Number of Channels of the Input Predictor Signals\n",
    "        # problem_type: Regression or Classification\n",
    "        # output_nums: Number of Output Classes in Classification mode and output features in Regression mode\n",
    "        # pooling: Choose either 'max' for MaxPooling or 'avg' for Averagepooling\n",
    "        # dropout_rate: If turned on, some layers will be dropped out randomly based on the selected proportion\n",
    "        # auxilliary_outputs: Two extra Auxullary outputs for the Inception models, acting like Deep Supervision\n",
    "        self.length = length\n",
    "        self.width = width\n",
    "        self.num_channel = num_channel\n",
    "        self.num_filters = num_filters\n",
    "        self.ratio = ratio\n",
    "        self.problem_type = problem_type\n",
    "        self.output_nums = output_nums\n",
    "        self.pooling = pooling\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.auxilliary_outputs = auxilliary_outputs\n",
    "\n",
    "    def MLP(self, x):\n",
    "        if self.pooling == 'avg':\n",
    "            x = tf.keras.layers.GlobalAveragePooling2D()(x)\n",
    "        elif self.pooling == 'max':\n",
    "            x = tf.keras.layers.GlobalMaxPooling2D()(x)\n",
    "        if self.dropout_rate:\n",
    "            x = tf.keras.layers.Dropout(self.dropout_rate)(x)\n",
    "        # Final Dense Outputting Layer for the outputs\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        outputs = tf.keras.layers.Dense(self.output_nums, activation='linear')(x)\n",
    "        if self.problem_type == 'Classification':\n",
    "            outputs = tf.keras.layers.Dense(self.output_nums, activation='softmax')(x)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def SEInception_v1(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, self.num_filters, 7, strides=2)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Conv_2D_Block(x, self.num_filters, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 3, 3)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Inceptionv1_Module(x, 64, 96, 128, 16, 32, 32, 1)  # Inception Block 1\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 128, 128, 192, 32, 96, 64, 2)  # Inception Block 2\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Inceptionv1_Module(x, 192, 96, 208, 16, 48, 64, 3)  # Inception Block 3\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 160, 112, 224, 24, 64, 64, 4)  # Inception Block 4\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 128, 128, 256, 24, 64, 64, 5)  # Inception Block 5\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 112, 144, 288, 32, 64, 64, 6)  # Inception Block 6\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 7)  # Inception Block 7\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Inceptionv1_Module(x, 256, 160, 320, 32, 128, 128, 8)  # Inception Block 8\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv1_Module(x, 384, 192, 384, 48, 128, 128, 9)  # Inception Block 9\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v1')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v2(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem: 56 x 64\n",
    "        x = tf.keras.layers.SeparableConv2D(self.num_filters, kernel_size=7, strides=(2, 2), depth_multiplier=1, padding='same')(inputs)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Inceptionv2_Module(x, 64, 64, 64, 64, 96, 96, 32, 1)  # Inception Block 1: 28 x 192\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 64, 64, 96, 64, 96, 96, 64, 2)  # Inception Block 2: 28 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 128, 160, 64, 96, 96, 1)  # Reduction Block 1: 28 x 320\n",
    "\n",
    "        x = Inceptionv2_Module(x, 224, 64, 96, 96, 128, 128, 128, 3)  # Inception Block 3: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 192, 96, 128, 96, 128, 128, 128, 4)  # Inception Block 4: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 160, 128, 160, 128, 160, 160, 96, 5)  # Inception Block 5: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 96, 128, 192, 160, 192, 192, 96, 6)  # Inception Block 6: 14 x 576\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 128, 192, 192, 256, 256, 2)  # Reduction Block 2: 14 x 576\n",
    "\n",
    "        x = Inceptionv2_Module(x, 352, 192, 320, 160, 224, 224, 128, 7)  # Inception Block 7: 7 x 1024\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inceptionv2_Module(x, 352, 192, 320, 192, 224, 224, 128, 8)  # Inception Block 8: 7 x 1024\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v2')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v3(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, self.num_filters, 3, strides=2, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters, 3, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2, 3)\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        x = Conv_2D_Block(x, self.num_filters * 2.5, 1, padding='valid')\n",
    "        x = Conv_2D_Block(x, self.num_filters * 6, 3, padding='valid')\n",
    "        x = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "\n",
    "        # 3x Inception-A Blocks\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 32, 1)  # Inception-A Block 1: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 2)  # Inception-A Block 2: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_A(x, 64, 48, 64, 64, 96, 96, 64, 3)  # Inception-A Block 3: 35 x 256\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 64, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 64, 384, 64, 96, 96, 1)  # Reduction Block 1: 17 x 768\n",
    "\n",
    "        # 4x Inception-B Blocks\n",
    "        x = Inception_Module_B(x, 192, 128, 192, 128, 128, 192, 192, 1)  # Inception-B Block 1: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 2)  # Inception-B Block 2: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 160, 192, 160, 160, 192, 192, 3)  # Inception-B Block 3: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_B(x, 192, 192, 192, 192, 192, 192, 192, 4)  # Inception-B Block 4: 17 x 768\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 192, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_B(x, 192, 320, 192, 192, 192, 2)  # Reduction Block 2: 8 x 1280\n",
    "\n",
    "        # 2x Inception-C Blocks: 8 x 2048\n",
    "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 1)  # Inception-C Block 1: 8 x 2048\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "        x = Inception_Module_C(x, 320, 384, 384, 448, 384, 384, 192, 2)  # Inception-C Block 2: 8 x 2048\n",
    "        x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v3')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v3')\n",
    "\n",
    "        return model\n",
    "\n",
    "    def SEInception_v4(self):\n",
    "        inputs = tf.keras.Input((self.length, self.width, self.num_channel))  # The input tensor\n",
    "        # Stem\n",
    "        x = Conv_2D_Block(inputs, 32, 3, strides=2, padding='valid')\n",
    "        x = Conv_2D_Block(x, 32, 3, padding='valid')\n",
    "        x = Conv_2D_Block(x, 64, 3)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 96, 3, strides=2, padding='valid')\n",
    "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(2, 2))(x)\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 64, 1)\n",
    "        branch1 = Conv_2D_Block(branch1, 96, 3, padding='valid')\n",
    "        branch2 = Conv_2D_Block(x, 64, 1)\n",
    "        branch2 = Conv_2D_Block(branch2, 64, 7)\n",
    "        branch2 = Conv_2D_Block(branch2, 96, 3, padding='valid')\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        branch1 = Conv_2D_Block(x, 192, 3, padding='valid')\n",
    "        branch2 = tf.keras.layers.MaxPooling2D(pool_size=(3, 3), strides=(1, 1))(x)\n",
    "        x = tf.keras.layers.concatenate([branch1, branch2], axis=-1)\n",
    "\n",
    "        # 4x Inception-A Blocks - 35 x 256\n",
    "        for i in range(4):\n",
    "            x = Inception_Module_A(x, 96, 64, 96, 64, 96, 96, 96, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_0 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 0\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 96, 1)\n",
    "            aux_output_0 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_A(x, 64, 384, 192, 224, 256, 1)  # Reduction Block 1: 17 x 768\n",
    "\n",
    "        # 7x Inception-B Blocks - 17 x 768\n",
    "        for i in range(7):\n",
    "            x = Inception_Module_B(x, 384, 192, 256, 192, 224, 256, 128, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        aux_output_1 = []\n",
    "        if self.auxilliary_outputs:\n",
    "            # Auxilliary Output 1\n",
    "            aux_pool = tf.keras.layers.AveragePooling2D(pool_size=(5, 5), strides=(3, 3), padding='valid')(x)\n",
    "            aux_conv = Conv_2D_Block(aux_pool, 128, 1)\n",
    "            aux_output_1 = self.MLP(aux_conv)\n",
    "\n",
    "        x = Reduction_Block_B(x, 192, 192, 256, 320, 320, 2)  # Reduction Block 2: 8 x 1280\n",
    "\n",
    "        # 3x Inception-C Blocks: 8 x 2048\n",
    "        for i in range(3):\n",
    "            x = Inception_Module_C(x, 256, 384, 512, 384, 512, 512, 256, i)\n",
    "            x = SE_Block(x, int(np.shape(x)[-1]), self.ratio)\n",
    "\n",
    "        # Final Dense MLP Layer for the outputs\n",
    "        final_output = self.MLP(x)\n",
    "        # Create model.\n",
    "        model = tf.keras.Model(inputs, final_output, name='Inception_v4')\n",
    "        if self.auxilliary_outputs:\n",
    "            model = tf.keras.layers.Model(inputs, outputs=[final_output, aux_output_0, aux_output_1], name='Inception_v4')\n",
    "\n",
    "        return model\n",
    "\n",
    "\n",
    "\n",
    "accuracy_rank=[]\n",
    "precision_rank=[]\n",
    "recall_rank=[]\n",
    "fscore_rank=[]\n",
    "\n",
    "\n",
    "    \n",
    "def my_plots(folder_path,history,my_model):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation accuracy curve of \"+my_model+\".png\"\n",
    "    plt.savefig(folder_path+my_path)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "    #plt.ylim([-3, 3])\n",
    "    plt.yticks(np.arange(0, 1.1, 0.25))\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    my_path=\"training and validation loss curve of \"+my_model+\".png\"\n",
    "    plt.savefig(folder_path+my_path)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "from keras.utils import get_file\n",
    "\n",
    "def making_training_and_testing_data(full_data,full_label):\n",
    "    train_label=[]\n",
    "    for i in range(len(full_label)):\n",
    "        if full_label[i]==0:\n",
    "            train_label.append([0,1,0,0,0])\n",
    "        elif full_label[i]==1:\n",
    "            train_label.append([1,0,0,0,0])\n",
    "        elif full_label[i]==2:\n",
    "            train_label.append([0,0,1,0,0])\n",
    "        elif full_label[i]==3:\n",
    "            train_label.append([0,0,0,1,0])\n",
    "        elif full_label[i]==4:\n",
    "            train_label.append([0,0,0,0,1])\n",
    "    full_label=np.array(train_label)\n",
    "    return full_data,full_label\n",
    "def making_full_data(aug_cat1,aug_cat2,aug_cat3,aug_cat4,aug_cat5):\n",
    "    aug_cat1=shuffle(aug_cat1, random_state=0)\n",
    "    aug_cat2=shuffle(aug_cat2,random_state=0)\n",
    "    aug_cat3=shuffle(aug_cat3,random_state=0)\n",
    "    aug_cat4=shuffle(aug_cat4,random_state=0)\n",
    "    aug_cat5=shuffle(aug_cat5,random_state=0)\n",
    "    \n",
    "    aug_cat1_labels=[]\n",
    "    for i in range(len(aug_cat1)):\n",
    "        aug_cat1_labels.append(0)\n",
    "    print(np.shape(aug_cat1),np.shape(aug_cat1_labels))\n",
    "    aug_cat2_labels=[]\n",
    "    for i in range(len(aug_cat2)):\n",
    "        aug_cat2_labels.append(1)\n",
    "    print(np.shape(aug_cat2),np.shape(aug_cat2_labels))\n",
    "    aug_cat3_labels=[]\n",
    "    for i in range(len(aug_cat3)):\n",
    "        aug_cat3_labels.append(2)\n",
    "    print(np.shape(aug_cat3),np.shape(aug_cat3_labels))  \n",
    "    aug_cat4_labels=[]\n",
    "    for i in range(len(aug_cat4)):\n",
    "        aug_cat4_labels.append(3)\n",
    "    print(np.shape(aug_cat4),np.shape(aug_cat4_labels))  \n",
    "    aug_cat5_labels=[]\n",
    "    for i in range(len(aug_cat5)):\n",
    "        aug_cat5_labels.append(4)\n",
    "    print(np.shape(aug_cat5),np.shape(aug_cat5_labels)) \n",
    "\n",
    "    full_data=[]\n",
    "    full_label=[]\n",
    "    for i in range(len(aug_cat1)):\n",
    "        full_data.append(aug_cat1[i])\n",
    "        full_label.append(aug_cat1_labels[i])\n",
    "    for i in range(len(aug_cat2)):\n",
    "        full_data.append(aug_cat2[i])\n",
    "        full_label.append(aug_cat2_labels[i])\n",
    "    for i in range(len(aug_cat3)):\n",
    "        full_data.append(aug_cat3[i])\n",
    "        full_label.append(aug_cat3_labels[i])\n",
    "    for i in range(len(aug_cat4)):\n",
    "        full_data.append(aug_cat4[i])\n",
    "        full_label.append(aug_cat4_labels[i])\n",
    "    for i in range(len(aug_cat5)):\n",
    "        full_data.append(aug_cat5[i])\n",
    "        full_label.append(aug_cat5_labels[i])\n",
    "        \n",
    "    full_data=np.array(full_data)\n",
    "    full_label=np.array(full_label)\n",
    "    \n",
    "    full_data=shuffle(full_data,random_state=0)\n",
    "    full_label=shuffle(full_label,random_state=0)\n",
    "    \n",
    "    return full_data,full_label\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "#from keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.resnet50 import preprocess_input as rp\n",
    "from classification_models.keras import Classifiers\n",
    "from tensorflow.keras.applications.xception import preprocess_input as xp\n",
    "import sklearn.metrics as metrics\n",
    "import math\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "\n",
    "#Fuzzy Rank-based Ensemble:\n",
    "def getScore(model,test_imgs):\n",
    "  res = model.predict(test_imgs)\n",
    "  return res \n",
    "\n",
    "def generateRank1(score,class_no):\n",
    "  rank = np.zeros([class_no,1])\n",
    "  scores = np.zeros([class_no,1])\n",
    "  scores = score\n",
    "  for i in range(class_no):\n",
    "      rank[i] = 1 - np.exp(-((scores[i]-1)**2)/2.0)\n",
    "  return rank\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "def generateRank2(score,class_no):\n",
    "  rank = np.zeros([class_no,1])\n",
    "  scores = np.zeros([class_no,1])\n",
    "  scores = score\n",
    "  for i in range(class_no):\n",
    "      rank[i] = 2*(1-sigmoid((scores[i]-1)**2))\n",
    "  return rank\n",
    "#[1,2,3]+[2,3,4]=[3,5,7] [0,1,0]  [1,0,0]  [0,0,1]\n",
    "def doFusion(res1,res2,res3,label,class_no):\n",
    "  cnt = 0\n",
    "  id = []\n",
    "  for i in range(len(res1)):\n",
    "      rank1 = generateRank1(res1[i],class_no)*generateRank2(res1[i],class_no)\n",
    "      rank2 = generateRank1(res2[i],class_no)*generateRank2(res2[i],class_no)\n",
    "      rank3 = generateRank1(res3[i],class_no)*generateRank2(res3[i],class_no)\n",
    "      rankSum = rank1 + rank2 + rank3 #list\n",
    "      rankSum = np.array(rankSum)\n",
    "#       scoreSum = 1 - (res1[i] + res2[i] + res3[i])/3\n",
    "#       scoreSum = np.array(scoreSum)\n",
    "      \n",
    "#       fusedScore = (rankSum.T)*scoreSum\n",
    "      cls = np.argmin(rankSum)\n",
    "   \n",
    "      if cls<class_no and label[i][cls]== 1:\n",
    "          cnt += 1\n",
    "      id.append(cls)\n",
    "  print(cnt/len(res1))\n",
    "  return id   \n",
    "    \n",
    "def rank_fuzzy(fold,X_test,y_test,model1,model2,model3):\n",
    "    #(model_counter,val_full_data,val_full_label,val_full_data_incep,val_full_label_incep,curr_densenet,curr_mobilenet,curr_inception)\n",
    "    model1 = model1\n",
    "\n",
    "    model2 = model2\n",
    "    model3 = model3\n",
    "    testing_data=X_test\n",
    "    res1 = model1.predict(testing_data)\n",
    "    model1.evaluate(testing_data, y_test)\n",
    "    num_classes=5\n",
    "    res2 = model2.predict(testing_data)\n",
    "    model2.evaluate(testing_data, y_test)\n",
    "    testing_data=X_test\n",
    "    res3=model3.predict(testing_data)\n",
    "    model3.evaluate(testing_data, y_test)\n",
    "    predictedClass = doFusion(res1,res2,res3,y_test,class_no=num_classes)\n",
    "    labels = np.argmax(y_test,axis=-1)\n",
    "    count=0\n",
    "    for i in range(len(labels)):\n",
    "        if predictedClass[i]==labels[i]:\n",
    "            count+=1\n",
    "    accuracy= (count/len(labels))\n",
    "    accuracy_rank.append(accuracy)\n",
    "    accuracy= (count/len(labels))*100\n",
    "    predictions=predictedClass\n",
    "    print(\"Accuracy of rank based fuzzy ensemble:\" + str(accuracy)+\"%\")\n",
    "    y_label=np.argmax(y_test,axis=1).tolist()\n",
    "   \n",
    "    cm=metrics.confusion_matrix(predictions,y_label)\n",
    "    cm_df = pd.DataFrame(cm)\n",
    "    \n",
    "#     plt.figure(figsize=(3,3))\n",
    "    sns.heatmap(cm_df, annot=True,fmt='g',annot_kws={'size':20})\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('Actual Values')\n",
    "    plt.xlabel('Predicted Values')\n",
    "    plt.savefig(\"confusion_matrix_rank_based_ensemble_with_sigmoid\"+str(fold)+\".png\")\n",
    "    plt.show()\n",
    "    print(\"micro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro precision score of Rank Based Fuzzy:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    precision_rank.append(metrics.precision_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='micro'))\n",
    "    print(\"macro recall score of Rank Based Fuzzy:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    recall_rank.append(metrics.recall_score(predictions,y_label,average='macro'))\n",
    "    print(\"micro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='micro'))\n",
    "    print(\"macro f1 score of Rank Based Fuzzy:\",f1_score(predictions, y_label, average='macro'))\n",
    "    fscore_rank.append(metrics.f1_score(predictions, y_label, average='macro'))\n",
    "    return y_label,predictions,(count/len(labels))\n",
    "\n",
    "if __name__ == '__main__':  #straight away go to this\n",
    "    cat1_dir = \"im_Dyskeratotic/CROPPED/\" \n",
    "    dir1 = os.path.join(cat1_dir,\"*.bmp\")\n",
    "    dir2 = os.path.join(cat1_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(cat1_dir,\"*.jpg\")\n",
    "    cat1_files = glob.glob(dir)\n",
    "    cat1_1 = glob.glob(dir1)\n",
    "    cat1_2 = glob.glob(dir2)\n",
    "    cat1_files.extend(cat1_1)\n",
    "    cat1_files.extend(cat1_2)\n",
    "\n",
    "    cat2_dir = \"im_Koilocytotic/CROPPED/\"  \n",
    "    dir1 = os.path.join(cat2_dir,\"*.bmp\")\n",
    "    dir = os.path.join(cat2_dir,\"*.jpg\")\n",
    "    dir2 = os.path.join(cat2_dir,\"*.jpeg\")\n",
    "    cat2_files = glob.glob(dir)\n",
    "    cat2_files2 = glob.glob(dir2)\n",
    "    cat2_files1 = glob.glob(dir1)\n",
    "    cat2_files.extend(cat2_files2)\n",
    "    cat2_files.extend(cat2_files1)\n",
    "\n",
    "    cat3_dir = \"im_Metaplastic/CROPPED/\" \n",
    "    dir1 = os.path.join(cat3_dir,\"*.bmp\")\n",
    "    dir2 = os.path.join(cat3_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(cat3_dir,\"*.jpg\")\n",
    "    cat3_files = glob.glob(dir)\n",
    "    cat3_1 = glob.glob(dir1)\n",
    "    cat3_2 = glob.glob(dir2)\n",
    "    cat3_files.extend(cat3_1)\n",
    "    cat3_files.extend(cat3_2)\n",
    "\n",
    "    cat4_dir = \"im_Parabasal/CROPPED/\" \n",
    "    dir1 = os.path.join(cat4_dir,\"*.bmp\")\n",
    "    dir2 = os.path.join(cat4_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(cat4_dir,\"*.jpg\")\n",
    "    cat4_files = glob.glob(dir)\n",
    "    cat4_1 = glob.glob(dir1)\n",
    "    cat4_2 = glob.glob(dir2)\n",
    "    cat4_files.extend(cat4_1)\n",
    "    cat4_files.extend(cat4_2)\n",
    "    \n",
    "    cat5_dir = \"im_Superficial-Intermediate/CROPPED/\" \n",
    "    dir1 = os.path.join(cat5_dir,\"*.bmp\")\n",
    "    dir2 = os.path.join(cat5_dir,\"*.jpeg\")\n",
    "    dir = os.path.join(cat5_dir,\"*.jpg\")\n",
    "    cat5_files = glob.glob(dir)\n",
    "    cat5_1 = glob.glob(dir1)\n",
    "    cat5_2 = glob.glob(dir2)\n",
    "    cat5_files.extend(cat5_1)\n",
    "    cat5_files.extend(cat5_2)\n",
    "    \n",
    "    cat1_files.sort()  \n",
    "    cat2_files.sort()  \n",
    "    cat3_files.sort()  \n",
    "    cat4_files.sort() \n",
    "    cat5_files.sort()  \n",
    "    cat1_files=shuffle(cat1_files,random_state=10)\n",
    "    cat2_files=shuffle(cat2_files,random_state=10)\n",
    "    cat3_files=shuffle(cat3_files,random_state=10)\n",
    "    cat4_files=shuffle(cat4_files,random_state=10)\n",
    "    cat5_files=shuffle(cat5_files,random_state=10)\n",
    "    \n",
    "    print(\"cat1_files:\",len(cat1_files))\n",
    "    print(\"cat2_files:\",len(cat2_files))\n",
    "    print(\"cat3_files:\",len(cat3_files))\n",
    "    print(\"cat4_files:\",len(cat4_files))\n",
    "    print(\"cat5_files:\",len(cat5_files))\n",
    "    \n",
    "    total_files=(len(cat1_files)+len(cat2_files)+len(cat3_files)+len(cat4_files))+len(cat5_files)\n",
    "    \n",
    "    temp_files=[]\n",
    "    temp_labels=[]\n",
    "    for i in range(len(cat3_files)):\n",
    "        temp_files.append(cat3_files[i])\n",
    "        temp_labels.append(0)\n",
    "    \n",
    "    for i in range(len(cat2_files)):\n",
    "        temp_files.append(cat2_files[i])\n",
    "        temp_labels.append(1)\n",
    "    \n",
    "    for i in range(len(cat1_files)):\n",
    "        temp_files.append(cat1_files[i])\n",
    "        temp_labels.append(2)\n",
    "        \n",
    "    for i in range(len(cat4_files)):\n",
    "        temp_files.append(cat4_files[i])\n",
    "        temp_labels.append(3)\n",
    "        \n",
    "    for i in range(len(cat5_files)):\n",
    "        temp_files.append(cat5_files[i])\n",
    "        temp_labels.append(4)\n",
    "        \n",
    "    temp_files=shuffle(temp_files,random_state=10)\n",
    "    temp_labels=shuffle(temp_labels,random_state=10)  \n",
    "    skf = StratifiedKFold(n_splits=5,shuffle=True, random_state=42)\n",
    "    model_counter=0\n",
    "    \n",
    "    early_stopping = EarlyStopping(monitor='val_loss', \n",
    "        patience=30, \n",
    "\n",
    "        min_delta=0.001, \n",
    "        mode='min')\n",
    "        \n",
    "       \n",
    "    model_counter=0    \n",
    "    accuracy_d=[]\n",
    "    precision_d=[]\n",
    "    recall_d=[]\n",
    "    f_score_d=[]\n",
    "    \n",
    "    temp_files=np.array(temp_files)\n",
    "    temp_labels=np.array(temp_labels)\n",
    "    \n",
    "\n",
    "    counter=-1\n",
    "    my_fold=0\n",
    "    for train_index, val_index in skf.split(temp_files, temp_labels):\n",
    "    # Split the data into training and validation sets\n",
    "        X_train, X_val = temp_files[train_index], temp_files[val_index]\n",
    "        y_train, y_val = temp_labels[train_index], temp_labels[val_index]\n",
    "        \n",
    "        \n",
    "    \n",
    "        #train\n",
    "        train_cat1_files=[]\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i]==2:\n",
    "                train_cat1_files.append(X_train[i])\n",
    "\n",
    "        train_cat2_files=[]\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i]==1:\n",
    "                train_cat2_files.append(X_train[i])\n",
    "\n",
    "        train_cat3_files=[]\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i]==0:\n",
    "                train_cat3_files.append(X_train[i])\n",
    "\n",
    "        train_cat4_files=[]\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i]==3:\n",
    "                train_cat4_files.append(X_train[i])\n",
    "                \n",
    "        \n",
    "        train_cat5_files=[]\n",
    "        for i in range(len(X_train)):\n",
    "            if y_train[i]==4:\n",
    "                train_cat5_files.append(X_train[i])\n",
    "        \n",
    "        \n",
    "        #val\n",
    "        val_cat1_files=[]\n",
    "        for i in range(len(X_val)):\n",
    "            if y_val[i]==2:\n",
    "                val_cat1_files.append(X_val[i])\n",
    "\n",
    "        val_cat2_files=[]\n",
    "        for i in range(len(X_val)):\n",
    "            if y_val[i]==1:\n",
    "                val_cat2_files.append(X_val[i])\n",
    "\n",
    "        val_cat3_files=[]\n",
    "        for i in range(len(X_val)):\n",
    "            if y_val[i]==0:\n",
    "                val_cat3_files.append(X_val[i])\n",
    "\n",
    "        val_cat4_files=[]\n",
    "        for i in range(len(X_val)):\n",
    "            if y_val[i]==3:\n",
    "                val_cat4_files.append(X_val[i])\n",
    "        \n",
    "        val_cat5_files=[]\n",
    "        for i in range(len(X_val)):\n",
    "            if y_val[i]==4:\n",
    "                val_cat5_files.append(X_val[i])\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "        val_aug_cat1,val_aug_cat2,val_aug_cat3,val_aug_cat4,val_aug_cat5=no_data_augmentation(val_cat1_files,val_cat2_files,val_cat3_files,val_cat4_files,val_cat5_files)\n",
    "        \n",
    "\n",
    "        val_full_data,val_full_label=making_full_data(val_aug_cat1,val_aug_cat2,val_aug_cat3,val_aug_cat4,val_aug_cat5)\n",
    "        \n",
    "        \n",
    "        #train_full_data,train_full_label= making_training_and_testing_data_train(train_full_data,train_full_label) #dividing full_data into train and test data\n",
    "        val_full_data,val_full_label=making_training_and_testing_data(val_full_data,val_full_label)\n",
    "        \n",
    "        #train_data=train_full_data\n",
    "        test_data=val_full_data\n",
    "       \n",
    "        Model=pickle.load(open('DenseNet169_model_'+str(model_no)+'.sav', 'rb'))\n",
    "        pred=Model.predict(test_data)\n",
    "        predictions = np.argmax(pred,axis = 1)\n",
    "        y_label=np.argmax(val_full_label,axis = 1)\n",
    "        print(\"Accuracy is:\",accuracy_score(predictions,y_label))\n",
    "        accuracy_d.append(accuracy_score(predictions,y_label))\n",
    "        print(\"Precision is:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        precision_d.append(metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        print(\"Recall is:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        recall_d.append(metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        print(\"F1 Score is:\",f1_score(predictions, y_label, average='macro'))\n",
    "        f_score_d.append(f1_score(predictions, y_label, average='macro'))\n",
    "        print(\"accuracy of model no. \"+str(model_counter)+\"is: \",Model.evaluate(val_full_data,val_full_label)[1])\n",
    "        curr_densenet=Model\n",
    "        \n",
    "        \n",
    "        Model = pickle.load(open('ResNet152V2_model_'+str(model_no)+'.sav', 'rb'))\n",
    "        pred=Model.predict(test_data)\n",
    "        predictions = np.argmax(pred,axis = 1)\n",
    "        y_label=np.argmax(val_full_label,axis = 1)\n",
    "        print(\"Accuracy is:\",accuracy_score(predictions,y_label))\n",
    "        accuracy_r.append(accuracy_score(predictions,y_label))\n",
    "        print(\"Precision is:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        precision_r.append(metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        print(\"Recall is:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        recall_r.append(metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        print(\"F1 Score is:\",f1_score(predictions, y_label, average='macro'))\n",
    "        f_score_r.append(f1_score(predictions, y_label, average='macro'))\n",
    "        print(\"accuracy of model no. \"+str(model_counter)+\"is: \",Model.evaluate(val_full_data,val_full_label)[1])\n",
    "        curr_resnet=Model\n",
    "\n",
    "\n",
    "        Model=pickle.load(open('SEInceptionV4_model_'+str(model_no)+'.sav', 'rb'))\n",
    "        pred=Model.predict(test_data)\n",
    "        predictions = np.argmax(pred,axis = 1)\n",
    "        y_label=np.argmax(val_full_label,axis = 1)\n",
    "        print(\"Accuracy is:\",accuracy_score(predictions,y_label))\n",
    "        accuracy_i.append(accuracy_score(predictions,y_label))\n",
    "        print(\"Precision is:\",metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        precision_i.append(metrics.precision_score(predictions,y_label,average='macro'))\n",
    "        print(\"Recall is:\",metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        recall_i.append(metrics.recall_score(predictions,y_label,average='macro'))\n",
    "        print(\"F1 Score is:\",f1_score(predictions, y_label, average='macro'))\n",
    "        f_score_i.append(f1_score(predictions, y_label, average='macro'))\n",
    "        print(\"accuracy of model no. \"+str(model_counter)+\"is: \",Model.evaluate(val_full_data,val_full_label)[1])\n",
    "        curr_inception=Model\n",
    "\n",
    "       \n",
    "        true_label,pred_rank,acc_rank = rank_fuzzy(model_counter,val_full_data,val_full_label,curr_densenet,curr_resnet,curr_inception)\n",
    "        model_counter+=1\n",
    "        model_no+=1\n",
    "\n",
    "\n",
    "    fold_number=[1,2,3,4,5]\n",
    "    zipped = list(zip(fold_number, accuracy_rank, precision_rank, recall_rank, fscore_rank ))\n",
    "\n",
    "    df = pd.DataFrame(zipped, columns=['Fold Number','Accuracy_rank', 'Precision_rank', 'Recall_rank', 'F_SCORE_rank'])\n",
    "    df.to_csv(\"ensembling_results_k_fold_results_rank_with_sigmoid.csv\")\n",
    "        \n",
    "    zipped = list(zip(fold_number,accuracy_d, precision_d, recall_d, f_score_d))\n",
    "\n",
    "    df = pd.DataFrame(zipped, columns=['Fold Number','Accuracy', 'Precision', 'Recall', 'F_SCORE'])\n",
    "    df.to_csv(\"densenet169_final_results.csv\")\n",
    "    \n",
    "\n",
    "    fold_number=[1,2,3,4,5]\n",
    "    \n",
    "    zipped = list(zip(fold_number,accuracy_r, precision_r, recall_r, f_score_r))\n",
    "\n",
    "    df = pd.DataFrame(zipped, columns=['Fold Number','Accuracy', 'Precision', 'Recall', 'F_SCORE'])\n",
    "    df.to_csv(\"ResNet152V2_final_k_fold_results.csv\")\n",
    "    \n",
    "\n",
    "    fold_number=[1,2,3,4,5]\n",
    "    zipped = list(zip(fold_number,accuracy_i, precision_i, recall_i, f_score_i))\n",
    "\n",
    "    df = pd.DataFrame(zipped, columns=['Fold Number','Accuracy', 'Precision', 'Recall', 'F_SCORE'])\n",
    "    df.to_csv(\"se_inception_v4_final_k_fold_results.csv\")\n",
    "    \n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
